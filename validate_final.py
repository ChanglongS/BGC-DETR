#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import argparse
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import warnings
from sklearn.metrics import f1_score, precision_score, recall_score

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from models.detr import DETR
from models.transformer import build_transformer
from data import BalancedClusterDataset
from util.misc import collate_fn
from engine import evaluate

warnings.filterwarnings("ignore")


def load_model_from_checkpoint(checkpoint_path, args):
    """
    ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹

    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        args: å‚æ•°é…ç½®

    Returns:
        model: åŠ è½½çš„æ¨¡å‹
        criterion: æŸå¤±å‡½æ•°
        postprocessors: åå¤„ç†å™¨
        epoch: è®­ç»ƒè½®æ•°
    """
    print(f"Loading checkpoint: {checkpoint_path}")

    # Load checkpoint
    try:
        # Try new PyTorch parameters
        try:
            checkpoint = torch.load(
                checkpoint_path, map_location='cpu', weights_only=False)
        except TypeError:
            # Use old method if weights_only not supported
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
    except Exception as e:
        print(f"Failed to load checkpoint: {e}")
        return None, None, None, None

    # ä½¿ç”¨æ£€æŸ¥ç‚¹ä¸­çš„å‚æ•°æ¥é‡å»ºæ¨¡å‹ï¼Œç¡®ä¿å‚æ•°ä¸€è‡´æ€§
    if 'args' in checkpoint:
        training_args = checkpoint['args']
        # å°†è®­ç»ƒæ—¶çš„å…³é”®å‚æ•°å¤åˆ¶åˆ°å½“å‰argsä¸­
        for key in ['num_classes', 'num_queries', 'hidden_dim', 'nheads',
                    'enc_layers', 'dec_layers', 'dim_feedforward', 'dropout',
                    'pre_norm', 'dilation', 'masks', 'aux_loss',
                    'focal_alpha', 'clip_max_norm', 'weight_decay', 'embed_dim',
                    'remove_difficult', 'set_cost_class', 'set_cost_bbox', 'set_cost_giou',
                    'ce_loss_coef', 'bbox_loss_coef', 'giou_loss_coef', 'cds_loss_coef', 'eos_coef']:
            if hasattr(training_args, key):
                setattr(args, key, getattr(training_args, key))

    # åˆ›å»ºæ¨¡å‹
    from models import build_model
    model, criterion, postprocessors = build_model(args)

    # åŠ è½½æ¨¡å‹æƒé‡
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
        epoch = checkpoint.get('epoch', 0)
    else:
        model.load_state_dict(checkpoint)
        epoch = 0

    model.eval()
    print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®­ç»ƒè½®æ•°: {epoch}")

    return model, criterion, postprocessors, epoch


def create_validation_dataset(args):
    """
    åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼Œå¼ºåˆ¶ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«æ˜ å°„

    Args:
        args: å‚æ•°é…ç½®

    Returns:
        dataset: éªŒè¯æ•°æ®é›†
        dataloader: æ•°æ®åŠ è½½å™¨
    """
    print("æ­£åœ¨åˆ›å»ºéªŒè¯æ•°æ®é›†...")

    # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨è®­ç»ƒæ—¶çš„ç±»åˆ«æ˜ å°„
    # è®­ç»ƒæ—¶çš„ç±»åˆ«æ˜ å°„ï¼ˆæ¨¡å‹æœŸæœ›çš„ï¼‰
    fixed_label2id = {
        "NRPS": 0,
        "PKS": 1,
        "other": 2,
        "ribosomal": 3,
        "saccharide": 4,
        "terpene": 5,
        "no-object": 6
    }

    # ğŸ”¥ å¤„ç†éªŒè¯æ•°æ®é›†ä¸­çš„é¢å¤–ç±»åˆ«
    # å°†RiPPæ˜ å°„åˆ°otherç±»ï¼ˆå› ä¸ºè®­ç»ƒæ—¶æ²¡æœ‰RiPPï¼‰
    print("âš ï¸  æ£€æµ‹åˆ°éªŒè¯æ•°æ®é›†åŒ…å«è®­ç»ƒæ—¶æ²¡æœ‰çš„ç±»åˆ«:")
    print("  RiPP -> æ˜ å°„åˆ° other (ç´¢å¼•2)")

    # åˆ›å»ºæ•°æ®æ˜ å°„è½¬æ¢å‡½æ•°
    def convert_type_to_train_mapping(type_name):
        if type_name == "RiPP":
            return "other"  # å°†RiPPæ˜ å°„åˆ°other
        elif type_name in fixed_label2id:
            return type_name  # ä¿æŒåŸæœ‰æ˜ å°„
        else:
            print(f"âš ï¸  æœªçŸ¥ç±»åˆ«: {type_name} -> æ˜ å°„åˆ° other")
            return "other"  # æœªçŸ¥ç±»åˆ«ä¹Ÿæ˜ å°„åˆ°other

    print(f"ä½¿ç”¨å›ºå®šç±»åˆ«æ˜ å°„: {fixed_label2id}")

    # åˆ›å»ºéªŒè¯æ•°æ®é›†
    val_dataset = BalancedClusterDataset(
        mapping=args.mapping_json,
        emb_dir=args.emb_dir,
        max_tokens=args.max_tokens,
        balance_strategy='none',  # éªŒè¯æ—¶ä¸éœ€è¦å¹³è¡¡
    )

    # ğŸ”¥ å¼ºåˆ¶è¦†ç›–ç±»åˆ«æ˜ å°„
    val_dataset.label2id = fixed_label2id
    val_dataset.num_classes = len(fixed_label2id)

    # ğŸ”¥ åˆ›å»ºid2labelæ˜ å°„
    val_dataset.id2label = {v: k for k, v in fixed_label2id.items()}

    print(f"éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:")
    val_types = []
    with open(args.mapping_json, 'r') as f:
        mapping_data = json.load(f)
        for bgc_id, regions in mapping_data.items():
            # ç©ºæ ·æœ¬è®¡ä¸º no-object
            if not regions:
                val_types.append('no-object')
                continue
            for region in regions:
                val_types.append(region['type'])

    from collections import Counter
    type_counts = Counter(val_types)
    for cls, count in sorted(type_counts.items()):
        print(f"  {cls}: {count}ä¸ª")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    val_dataloader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn,
        drop_last=False
    )

    print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"Batchæ•°é‡: {len(val_dataloader)}")
    print(f"ç±»åˆ«æ˜ å°„: {val_dataset.label2id}")

    return val_dataset, val_dataloader


def collect_detailed_predictions(model, data_loader, device, class_mapping, output_file, model_info=None, max_tokens=128, sample_level_fp=False):
    """
    æ”¶é›†æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†é¢„æµ‹ä¿¡æ¯å¹¶ä¿å­˜åˆ°æ–‡ä»¶

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        data_loader: æ•°æ®åŠ è½½å™¨
        device: è®¡ç®—è®¾å¤‡
        class_mapping: ç±»åˆ«æ˜ å°„å­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        model_info: æ¨¡å‹ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹åç§°ã€checkpointè·¯å¾„ç­‰
        max_tokens: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œç”¨äºçœŸå®æ¡†åå½’ä¸€åŒ–
        sample_level_fp: æ˜¯å¦æŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—å‡é˜³æ€§ï¼ˆæ¨èç”¨äºè´Ÿæ ·æœ¬å¤„ç†ï¼‰
    """
    print(f"æ­£åœ¨æ”¶é›†è¯¦ç»†é¢„æµ‹ä¿¡æ¯ï¼Œä¿å­˜åˆ°: {output_file}")

    model.eval()
    detailed_results = {
        'model_info': model_info or {},
        'samples': []
    }

    # ç»Ÿè®¡å˜é‡
    total_samples = 0
    negative_samples = 0
    negative_samples_with_fp = 0
    total_fp_count = 0  # æŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—çš„FPæ€»æ•°

    with torch.no_grad():
        for batch_idx, (samples, targets) in enumerate(tqdm(data_loader, desc="æ”¶é›†é¢„æµ‹ä¿¡æ¯")):
            samples = samples.to(device)
            targets = [{k: v.to(device) for k, v in t.items()}
                       for t in targets]

            # æ¨¡å‹å‰å‘ä¼ æ’­
            outputs = model(samples)

            # è·å–é¢„æµ‹ç»“æœ
            # [batch_size, num_queries, num_classes]
            pred_logits = outputs['pred_logits']
            # [batch_size, num_queries, 2]
            pred_boxes = outputs['pred_boxes']

            # è®¡ç®—é¢„æµ‹æ¦‚ç‡å’Œç±»åˆ«
            pred_probs = torch.softmax(pred_logits, dim=-1)
            pred_scores, pred_labels = pred_probs[..., :-1].max(-1)  # æ’é™¤èƒŒæ™¯ç±»

            batch_size = pred_logits.shape[0]

            for i in range(batch_size):
                sample_result = {
                    'sample_id': batch_idx * data_loader.batch_size + i,
                    'predictions': [],
                    'ground_truth': [],
                    'is_negative_sample': False,
                    'has_positive_prediction': False
                }

                # è·å–åºåˆ—é•¿åº¦ç”¨äºåå½’ä¸€åŒ–
                seq_len = targets[i].get('size', 128)
                if isinstance(seq_len, torch.Tensor):
                    if seq_len.numel() == 1:
                        seq_len = seq_len.item()
                    else:
                        seq_len = seq_len[0].item()

                # è·å–max_tokensç”¨äºçœŸå®æ¡†åå½’ä¸€åŒ–
                # max_tokens = 128  # é»˜è®¤å€¼ï¼Œåº”è¯¥ä¸æ•°æ®åŠ è½½å™¨ä¸­çš„max_tokensä¸€è‡´

                # æ”¶é›†é¢„æµ‹æ¡†ä¿¡æ¯
                positive_pred_count = 0  # ç»Ÿè®¡æ­£æ ·æœ¬é¢„æµ‹æ•°é‡
                for j in range(pred_logits.shape[1]):  # num_queries
                    pred_label = pred_labels[i, j].item()
                    pred_score = pred_scores[i, j].item()
                    pred_box = pred_boxes[i, j]

                    # åå½’ä¸€åŒ–åæ ‡
                    start_cds = int(pred_box[0].item() * seq_len)
                    end_cds = int(pred_box[1].item() * seq_len)

                    # è·å–ç±»åˆ«åç§°
                    pred_class_name = class_mapping.get(
                        pred_label, f"unknown_{pred_label}")

                    prediction_info = {
                        'query_id': j,
                        'class_id': pred_label,
                        'class_name': pred_class_name,
                        'confidence': pred_score,
                        'start_cds': start_cds,
                        'end_cds': end_cds,
                        'normalized_start': pred_box[0].item(),
                        'normalized_end': pred_box[1].item()
                    }
                    sample_result['predictions'].append(prediction_info)

                    # ç»Ÿè®¡æ­£æ ·æœ¬é¢„æµ‹
                    if pred_label != 6:  # ä¸æ˜¯no-object
                        positive_pred_count += 1

                # æ”¶é›†çœŸå®æ¡†ä¿¡æ¯
                gt_boxes = targets[i]['boxes']
                gt_labels = targets[i]['labels']

                # æ£€æŸ¥æ˜¯å¦ä¸ºè´Ÿæ ·æœ¬ï¼ˆæ²¡æœ‰çœŸå®æ¡†æˆ–åªæœ‰no-objectæ¡†ï¼‰
                is_negative_sample = True
                for j in range(len(gt_boxes)):
                    gt_label = gt_labels[j].item()
                    gt_box = gt_boxes[j]

                    # çœŸå®æ¡†åæ ‡æ˜¯ç”¨max_tokenså½’ä¸€åŒ–çš„ï¼Œéœ€è¦ç”¨max_tokensåå½’ä¸€åŒ–
                    gt_start_cds = int(gt_box[0].item() * max_tokens)
                    gt_end_cds = int(gt_box[1].item() * max_tokens)

                    # è·å–ç±»åˆ«åç§°
                    gt_class_name = class_mapping.get(
                        gt_label, f"unknown_{gt_label}")

                    gt_info = {
                        'gt_id': j,
                        'class_id': gt_label,
                        'class_name': gt_class_name,
                        'start_cds': gt_start_cds,
                        'end_cds': gt_end_cds,
                        'normalized_start': gt_box[0].item(),
                        'normalized_end': gt_box[1].item()
                    }
                    sample_result['ground_truth'].append(gt_info)

                    # æ£€æŸ¥æ˜¯å¦æœ‰éno-objectçš„çœŸå®æ¡†
                    if gt_label != 6:  # ä¸æ˜¯no-object
                        is_negative_sample = False

                # æ ‡è®°è´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬é¢„æµ‹
                sample_result['is_negative_sample'] = is_negative_sample
                sample_result['has_positive_prediction'] = positive_pred_count > 0

                # ç»Ÿè®¡FP
                if is_negative_sample:
                    negative_samples += 1
                    if positive_pred_count > 0:
                        negative_samples_with_fp += 1
                        if sample_level_fp:
                            total_fp_count += 1  # æŒ‰æ ·æœ¬çº§åˆ«ï¼ŒåªåŠ 1ä¸ªFP
                        else:
                            total_fp_count += positive_pred_count  # æŒ‰é¢„æµ‹æ¡†çº§åˆ«

                total_samples += 1
                detailed_results['samples'].append(sample_result)

    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)

    print(f"è¯¦ç»†é¢„æµ‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_file}")
    print(f"å…±å¤„ç†äº† {total_samples} ä¸ªæ ·æœ¬")

    # æ‰“å°è´Ÿæ ·æœ¬åˆ†æç»“æœ
    if negative_samples > 0:
        print(f"\nè´Ÿæ ·æœ¬åˆ†æ:")
        print(f"  æ€»è´Ÿæ ·æœ¬æ•°: {negative_samples}")
        print(f"  æœ‰å‡é˜³æ€§çš„è´Ÿæ ·æœ¬æ•°: {negative_samples_with_fp}")
        print(f"  è´Ÿæ ·æœ¬å‡é˜³æ€§ç‡: {negative_samples_with_fp/negative_samples:.2%}")

        if sample_level_fp:
            print(f"   æŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—: å‡é˜³æ€§æ•° = {total_fp_count}")
            print(f"   æ¨èæ–¹å¼: æ¯ä¸ªè´Ÿæ ·æœ¬æœ€å¤šè´¡çŒ®1ä¸ªFP")
        else:
            print(f"   æŒ‰é¢„æµ‹æ¡†çº§åˆ«è®¡ç®—: å‡é˜³æ€§æ•° = {total_fp_count}")
            print(f"   å»ºè®®ä½¿ç”¨ --sample_level_fp å‚æ•°æ¥æŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—å‡é˜³æ€§")
    else:
        print("æ²¡æœ‰å‘ç°è´Ÿæ ·æœ¬")


def main():
    parser = argparse.ArgumentParser('DETR Final Validation')

    # æ¨¡å‹å‚æ•° - éœ€è¦ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
    parser.add_argument('--num_classes', default=7, type=int,
                        help='ç±»åˆ«æ•°é‡(åŒ…æ‹¬no-objectç±»)')  # ä¿®æ”¹ï¼šä½¿ç”¨å›ºå®šå€¼
    parser.add_argument('--num_queries', default=3, type=int, help='æŸ¥è¯¢æ•°é‡')
    parser.add_argument('--hidden_dim', default=256, type=int, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--nheads', default=8, type=int, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--enc_layers', default=6, type=int, help='ç¼–ç å™¨å±‚æ•°')
    parser.add_argument('--dec_layers', default=6, type=int, help='è§£ç å™¨å±‚æ•°')
    parser.add_argument('--dim_feedforward', default=2048,
                        type=int, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    parser.add_argument('--dropout', default=0.1, type=float, help='Dropoutç‡')
    parser.add_argument('--aux_loss', action='store_true',
                        default=True, help='ä½¿ç”¨è¾…åŠ©æŸå¤±')
    parser.add_argument('--max_tokens', default=128, type=int, help='æœ€å¤§åºåˆ—é•¿åº¦')

    # æ·»åŠ å…¶ä»–å¿…éœ€çš„å‚æ•°
    parser.add_argument('--backbone', default='esm2_t33_650M_UR50D', type=str)
    parser.add_argument('--position_embedding', default='sine', type=str)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--masks', action='store_true', default=False)
    parser.add_argument('--dilation', action='store_true', default=False)
    parser.add_argument('--ce_loss_coef', default=2, type=float)
    parser.add_argument('--bbox_loss_coef', default=5, type=float)
    parser.add_argument('--giou_loss_coef', default=2, type=float)
    parser.add_argument('--eos_coef', default=0.01,
                        type=float)  # no-objectç±»æƒé‡ç³»æ•°
    parser.add_argument('--pre_norm', action='store_true',
                        default=False, help='æ˜¯å¦åœ¨æ³¨æ„åŠ›å‰è¿›è¡Œå½’ä¸€åŒ–')
    parser.add_argument('--set_cost_class', default=1, type=float)
    parser.add_argument('--set_cost_bbox', default=5, type=float)
    parser.add_argument('--set_cost_giou', default=2, type=float)
    parser.add_argument('--focal_alpha', default=0.9, type=float)
    parser.add_argument('--clip_max_norm', default=0.1, type=float)
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--embed_dim', default=256, type=int)
    parser.add_argument('--remove_difficult',
                        action='store_true', default=False)

    # æ•°æ®å‚æ•°
    parser.add_argument('--mapping_json', default='data/test/bgc_mapping.json',
                        help='éªŒè¯é›†BGCæ˜ å°„æ–‡ä»¶ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰')
    parser.add_argument('--mapping_dir', default=None,
                        help='éªŒè¯é›†BGCæ˜ å°„ç›®å½•ï¼ˆå¤šæ–‡ä»¶/ä¹åŸºå› ç»„æ¨¡å¼ï¼‰')
    parser.add_argument('--emb_dir', default='data/test/embeddings',
                        help='éªŒè¯é›†åµŒå…¥æ–‡ä»¶ç›®å½•')
    parser.add_argument('--batch_size', default=8, type=int, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num_workers', default=2, type=int, help='æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°')

    # è¯„ä¼°å‚æ•°
    parser.add_argument('--device', default='cuda', help='ä½¿ç”¨è®¾å¤‡')
    parser.add_argument('--output_dir', default='./outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--validation_output_dir', default='./my_validation_results',
                        help='éªŒè¯ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--no_confidence_filter', action='store_true',
                        help='ç¦ç”¨ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œè®¾ç½®IoUé˜ˆå€¼ä¸º0.1ï¼Œæ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹ç»“æœ')
    parser.add_argument('--localization_only', action='store_true',
                        help='åªè®¡ç®—å®šä½ä»»åŠ¡ï¼Œå¿½ç•¥åˆ†ç±»ï¼ŒåŒ¹é…æ—¶åªè€ƒè™‘IoUä¸è€ƒè™‘ç±»åˆ«')

    # æ–°å¢ï¼šè¯¦ç»†é¢„æµ‹ä¿¡æ¯ä¿å­˜å‚æ•°
    parser.add_argument('--save_detailed_predictions', action='store_true',
                        help='ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†é¢„æµ‹ä¿¡æ¯åˆ°æ–‡ä»¶')
    parser.add_argument('--detailed_predictions_file', default='detailed_predictions.json',
                        help='è¯¦ç»†é¢„æµ‹ä¿¡æ¯ä¿å­˜æ–‡ä»¶å')
    parser.add_argument('--sample_level_fp', action='store_true',
                        help='æŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—å‡é˜³æ€§ï¼Œè€Œä¸æ˜¯æŒ‰é¢„æµ‹æ¡†æ•°é‡')

    args = parser.parse_args()

    print("=== DETR æœ€ç»ˆéªŒè¯ ===")
    print(f"ä½¿ç”¨è®¾å¤‡: {args.device}")
    print(f"éªŒè¯é›†æ˜ å°„æ–‡ä»¶: {args.mapping_json}")
    print(f"éªŒè¯é›†åµŒå…¥ç›®å½•: {args.emb_dir}")
    print(f"éªŒè¯ç»“æœè¾“å‡ºç›®å½•: {args.validation_output_dir}")
    print(f"è¯¦ç»†ç»“æœä¿å­˜ç›®å½•: {args.validation_output_dir}")
    print(f"æ¨¡å‹å‚æ•°: ç±»åˆ«æ•°={args.num_classes}, æŸ¥è¯¢æ•°={args.num_queries}")
    if args.localization_only:
        print(" å®šä½ä»»åŠ¡æ¨¡å¼: å·²å¯ç”¨ (åªè®¡ç®—å®šä½ï¼Œå¿½ç•¥åˆ†ç±»)")
        print(" åŒ¹é…ç­–ç•¥: åªè€ƒè™‘IoUï¼Œä¸è€ƒè™‘ç±»åˆ«")
    if args.no_confidence_filter:
        print(" ç½®ä¿¡åº¦è¿‡æ»¤: å·²ç¦ç”¨ (æ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹)")
        print(" IoUé˜ˆå€¼: 0.1 (æ›´å®½æ¾çš„åŒ¹é…æ¡ä»¶)")
    else:
        print(" ç½®ä¿¡åº¦è¿‡æ»¤: å·²å¯ç”¨ (è¯„ä¼°é˜ˆå€¼=0.3, åå¤„ç†é˜ˆå€¼=0.05)")
        print(" IoUé˜ˆå€¼: 0.5 (æ ‡å‡†åŒ¹é…æ¡ä»¶)")

    if args.save_detailed_predictions:
        print(" è¯¦ç»†é¢„æµ‹ä¿¡æ¯ä¿å­˜: å·²å¯ç”¨")
        print(f" ä¿å­˜æ–‡ä»¶: {args.detailed_predictions_file}")

    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"å®é™…ä½¿ç”¨è®¾å¤‡: {device}")

    # å¦‚æœæä¾›äº†mappingç›®å½•ï¼Œåˆ™å¯ç”¨å¤šåŸºå› ç»„è¯„ä¼°æ¨¡å¼
    if args.mapping_dir is not None:
        if not os.path.isdir(args.mapping_dir):
            print(f"é”™è¯¯: æ˜ å°„ç›®å½•ä¸å­˜åœ¨: {args.mapping_dir}")
            return
        mapping_files = [
            os.path.join(args.mapping_dir, f)
            for f in sorted(os.listdir(args.mapping_dir))
            if f.endswith('.json')
        ]
        if not mapping_files:
            print(f"é”™è¯¯: æ˜ å°„ç›®å½•ä¸­æœªæ‰¾åˆ°jsonæ–‡ä»¶: {args.mapping_dir}")
            return
        print(f"æ£€æµ‹åˆ°{len(mapping_files)}ä¸ªåŸºå› ç»„æ˜ å°„æ–‡ä»¶:")
        for mf in mapping_files:
            print(f"  - {os.path.basename(mf)}")
        multi_genome_mode = True
    else:
        multi_genome_mode = False
        # åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼‰
        val_dataset, val_dataloader = create_validation_dataset(args)
        print(f" ä½¿ç”¨æŒ‡å®šçš„ç±»åˆ«æ•°é‡: {args.num_classes}")
        print(f" æ•°æ®é›†ç±»åˆ«æ˜ å°„: {val_dataset.label2id}")
        print(f" æ•°æ®é›†å®é™…ç±»åˆ«æ•°: {val_dataset.num_classes}")

    # æ‰¾åˆ°æ‰€æœ‰foldçš„æ£€æŸ¥ç‚¹
    checkpoint_files = []
    for filename in os.listdir(args.output_dir):
        if filename.startswith('checkpoint_fold_') and filename.endswith('.pth'):
            checkpoint_files.append(os.path.join(args.output_dir, filename))

    checkpoint_files.sort()
    print(f"\næ‰¾åˆ° {len(checkpoint_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶:")
    for i, cp in enumerate(checkpoint_files):
        print(f"  {i+1}. {cp}")

    if not checkpoint_files:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶!")
        return

    # å¤šåŸºå› ç»„æ¨¡å¼ï¼šæŒ‰æ¨¡å‹ï¼ˆcheckpointï¼‰Ã— åŸºå› ç»„æ–‡ä»¶è¯„ä¼°
    if multi_genome_mode:
        for cp_idx, checkpoint_path in enumerate(checkpoint_files):
            model_name = os.path.splitext(os.path.basename(checkpoint_path))[0]
            model_out_dir = os.path.join(
                args.validation_output_dir, model_name)
            os.makedirs(model_out_dir, exist_ok=True)

            print(f"\n{'='*60}")
            print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
            print(f"è¾“å‡ºç›®å½•: {model_out_dir}")
            print(f"{'='*60}")

            # åŠ è½½æ¨¡å‹
            model, criterion, postprocessors, epoch = load_model_from_checkpoint(
                checkpoint_path, args)
            if model is None or criterion is None or postprocessors is None:
                print(f"è·³è¿‡æ— æ•ˆçš„æ£€æŸ¥ç‚¹: {checkpoint_path}")
                continue
            if epoch is None:
                epoch = 0
            model.to(device)

            # ç½®ä¿¡åº¦è¿‡æ»¤è®¾ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            original_eval_threshold = None
            original_iou_threshold = None
            original_forward = None
            if args.no_confidence_filter:
                print("âš ï¸  å·²ç¦ç”¨ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹ç»“æœ")
                print("âš ï¸  å·²è®¾ç½®IoUé˜ˆå€¼ä¸º0.1")
                import engine
                original_eval_threshold = engine.EVAL_CONFIDENCE_THRESHOLD
                original_iou_threshold = engine.IOU_THRESHOLD
                engine.EVAL_CONFIDENCE_THRESHOLD = 0.0
                engine.IOU_THRESHOLD = 0.1
                from models.detr import PostProcess
                original_forward = PostProcess.forward

                def no_filter_forward(self, outputs, target_sizes):
                    import torch
                    import torch.nn.functional as F
                    logits = outputs['pred_logits']
                    boxes = outputs['pred_boxes']
                    prob = F.softmax(logits, -1)
                    scores, labels = prob[..., :-1].max(-1)
                    if isinstance(target_sizes, (list, tuple)):
                        if len(target_sizes) > 0 and isinstance(target_sizes[0], dict):
                            size_values = []
                            for ts in target_sizes:
                                size_value = ts.get('size', 1000)
                                if isinstance(size_value, torch.Tensor):
                                    size_value = size_value.item() if size_value.numel(
                                    ) == 1 else size_value[0].item()
                                elif not isinstance(size_value, (int, float)):
                                    size_value = 1000
                                size_values.append(size_value)
                            num_cds = torch.tensor(
                                size_values, device=boxes.device)
                        else:
                            num_cds = torch.tensor(
                                target_sizes, device=boxes.device)
                    elif isinstance(target_sizes, dict):
                        size_value = target_sizes.get('size', 1000)
                        if isinstance(size_value, torch.Tensor):
                            size_value = size_value.item() if size_value.numel(
                            ) == 1 else size_value[0].item()
                        elif not isinstance(size_value, (int, float)):
                            size_value = 1000
                        num_cds = torch.tensor(
                            [size_value], device=boxes.device)
                    else:
                        num_cds = target_sizes[:, 0] if target_sizes.dim(
                        ) > 1 else target_sizes
                    if num_cds.dim() == 0:
                        num_cds = num_cds.unsqueeze(0)
                    boxes = boxes * num_cds[:, None, None]
                    boxes = torch.clamp(boxes, min=0)
                    boxes = boxes.round().long()
                    predictions = []
                    for i in range(len(scores)):
                        sample_predictions = []
                        for j in range(len(scores[i])):
                            prediction = {
                                'start_cds': boxes[i, j, 0].item(),
                                'end_cds': boxes[i, j, 1].item(),
                                'class': labels[i, j].item(),
                                'score': scores[i, j].item()
                            }
                            sample_predictions.append(prediction)
                        sample_predictions.sort(
                            key=lambda x: x['score'], reverse=True)
                        predictions.append(sample_predictions)
                    results = []
                    for i in range(len(scores)):
                        results.append(
                            {'scores': scores[i], 'labels': labels[i], 'boxes': boxes[i]})
                    return {'predictions': predictions, 'results': results}
                PostProcess.forward = no_filter_forward

            # éå†ä¹ä¸ªåŸºå› ç»„æ˜ å°„æ–‡ä»¶
            for mf in mapping_files:
                genome_name = os.path.splitext(os.path.basename(mf))[0]
                # ä¸´æ—¶è®¾ç½®æ˜ å°„æ–‡ä»¶
                args.mapping_json = mf
                # æ„å»ºæ•°æ®é›†/åŠ è½½å™¨
                val_dataset, val_dataloader = create_validation_dataset(args)

                # è¯„ä¼°ï¼ˆä¸ä¿å­˜è¯¦ç»†é¢„æµ‹ï¼‰
                val_results_dir = os.path.join(model_out_dir, '_internal')
                os.makedirs(val_results_dir, exist_ok=True)
                try:
                    eval_stats = evaluate(
                        model=model,
                        criterion=criterion,
                        postprocessors=postprocessors,
                        data_loader=val_dataloader,
                        device=device,
                        log_dir=val_results_dir,
                        epoch=epoch,
                        fold=1,
                        class_mapping=val_dataset.id2label,
                        localization_only=args.localization_only,
                        sample_level_fp=args.sample_level_fp
                    )
                except Exception as e:
                    print(f"è¯„ä¼°åŸºå› ç»„{genome_name}æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    continue

                # ä¿å­˜ç®€æ´æŒ‡æ ‡åˆ°æ¨¡å‹ç›®å½•ä¸‹ï¼ˆä¸€ä¸ªåŸºå› ç»„ä¸€ä¸ªæ–‡ä»¶ï¼‰
                out_metrics_path = os.path.join(
                    model_out_dir, f"{genome_name}_metrics.json")
                with open(out_metrics_path, 'w') as f:
                    json.dump(eval_stats, f, indent=2)
                print(
                    f"å·²ä¿å­˜ {model_name} åœ¨ {genome_name} ä¸Šçš„æŒ‡æ ‡: {out_metrics_path}")

            # æ¢å¤ç½®ä¿¡åº¦è¿‡æ»¤è®¾ç½®
            if args.no_confidence_filter and original_eval_threshold is not None:
                import engine
                engine.EVAL_CONFIDENCE_THRESHOLD = original_eval_threshold
                engine.IOU_THRESHOLD = original_iou_threshold
                if original_forward is not None:
                    from models.detr import PostProcess
                    PostProcess.forward = original_forward

        print("\néªŒè¯å®Œæˆ!")
        return

    # å•æ–‡ä»¶æ¨¡å¼ï¼šé€ä¸ªè¯„ä¼°æ¯ä¸ªfold
    all_results = []

    for i, checkpoint_path in enumerate(checkpoint_files):
        print(f"\n{'='*60}")
        print(f"è¯„ä¼° Fold {i+1}: {os.path.basename(checkpoint_path)}")
        print(f"{'='*60}")

        # åŠ è½½æ¨¡å‹
        model, criterion, postprocessors, epoch = load_model_from_checkpoint(
            checkpoint_path, args)
        if model is None or criterion is None or postprocessors is None:
            print(f"è·³è¿‡æ— æ•ˆçš„æ£€æŸ¥ç‚¹: {checkpoint_path}")
            continue

        # ç¡®ä¿epochä¸ä¸ºNone
        if epoch is None:
            epoch = 0

        model.to(device)

        # å¦‚æœå¯ç”¨äº†ç¦ç”¨ç½®ä¿¡åº¦è¿‡æ»¤é€‰é¡¹ï¼Œä¸´æ—¶ä¿®æ”¹å…¨å±€é˜ˆå€¼
        original_eval_threshold = None
        original_forward = None

        if args.no_confidence_filter:
            print("âš ï¸  å·²ç¦ç”¨ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹ç»“æœ")
            print("âš ï¸  å·²è®¾ç½®IoUé˜ˆå€¼ä¸º0.1")

            # ä¸´æ—¶ä¿®æ”¹engineä¸­çš„è¯„ä¼°ç½®ä¿¡åº¦é˜ˆå€¼å’ŒIoUé˜ˆå€¼
            import engine
            original_eval_threshold = engine.EVAL_CONFIDENCE_THRESHOLD
            original_iou_threshold = engine.IOU_THRESHOLD
            engine.EVAL_CONFIDENCE_THRESHOLD = 0.0
            engine.IOU_THRESHOLD = 0.3  # è®¾ç½®IoUé˜ˆå€¼

            # ä¸´æ—¶ä¿®æ”¹åå¤„ç†å™¨ä¸­çš„ç½®ä¿¡åº¦é˜ˆå€¼
            # é€šè¿‡monkey patchingä¿®æ”¹PostProcessçš„forwardæ–¹æ³•
            from models.detr import PostProcess
            original_forward = PostProcess.forward

            def no_filter_forward(self, outputs, target_sizes):
                """ä¸ä½¿ç”¨ç½®ä¿¡åº¦è¿‡æ»¤çš„åå¤„ç†ç‰ˆæœ¬"""
                import torch
                import torch.nn.functional as F

                logits = outputs['pred_logits']
                boxes = outputs['pred_boxes']

                prob = F.softmax(logits, -1)
                scores, labels = prob[..., :-1].max(-1)

                # å¤„ç†target_sizesçš„å„ç§æ ¼å¼
                if isinstance(target_sizes, (list, tuple)):
                    if len(target_sizes) > 0 and isinstance(target_sizes[0], dict):
                        size_values = []
                        for ts in target_sizes:
                            size_value = ts.get('size', 1000)
                            if isinstance(size_value, torch.Tensor):
                                size_value = size_value.item() if size_value.numel(
                                ) == 1 else size_value[0].item()
                            elif not isinstance(size_value, (int, float)):
                                size_value = 1000
                            size_values.append(size_value)
                        num_cds = torch.tensor(
                            size_values, device=boxes.device)
                    else:
                        num_cds = torch.tensor(
                            target_sizes, device=boxes.device)
                elif isinstance(target_sizes, dict):
                    size_value = target_sizes.get('size', 1000)
                    if isinstance(size_value, torch.Tensor):
                        size_value = size_value.item() if size_value.numel(
                        ) == 1 else size_value[0].item()
                    elif not isinstance(size_value, (int, float)):
                        size_value = 1000
                    num_cds = torch.tensor([size_value], device=boxes.device)
                else:
                    num_cds = target_sizes[:, 0] if target_sizes.dim(
                    ) > 1 else target_sizes

                if num_cds.dim() == 0:
                    num_cds = num_cds.unsqueeze(0)

                boxes = boxes * num_cds[:, None, None]
                boxes = torch.clamp(boxes, min=0)
                boxes = boxes.round().long()

                # æ„å»ºé¢„æµ‹ç»“æœï¼ˆä¸ä½¿ç”¨ç½®ä¿¡åº¦è¿‡æ»¤ï¼‰
                predictions = []
                for i in range(len(scores)):
                    sample_predictions = []
                    for j in range(len(scores[i])):
                        # ç§»é™¤ç½®ä¿¡åº¦è¿‡æ»¤ï¼Œæ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹
                        prediction = {
                            'start_cds': boxes[i, j, 0].item(),
                            'end_cds': boxes[i, j, 1].item(),
                            'class': labels[i, j].item(),
                            'score': scores[i, j].item()
                        }
                        sample_predictions.append(prediction)

                    # æŒ‰ç½®ä¿¡åº¦æ’åº
                    sample_predictions.sort(
                        key=lambda x: x['score'], reverse=True)
                    predictions.append(sample_predictions)

                # ä¿æŒå‘åå…¼å®¹çš„æ ¼å¼
                results = []
                for i in range(len(scores)):
                    results.append({
                        'scores': scores[i],
                        'labels': labels[i],
                        'boxes': boxes[i]
                    })

                return {'predictions': predictions, 'results': results}

            # æ›¿æ¢æ–¹æ³•
            PostProcess.forward = no_filter_forward

        # åˆ›å»ºéªŒè¯ç»“æœè¾“å‡ºç›®å½•
        os.makedirs(args.validation_output_dir, exist_ok=True)

        # åˆ›å»ºval_resultsç›®å½•ç”¨äºä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœ
        val_results_dir = os.path.join(
            args.validation_output_dir, 'val/val_results')
        os.makedirs(val_results_dir, exist_ok=True)

        # è¿è¡Œè¯„ä¼°
        try:
            eval_stats = evaluate(
                model=model,
                criterion=criterion,
                postprocessors=postprocessors,
                data_loader=val_dataloader,
                device=device,
                log_dir=val_results_dir,  # ä½¿ç”¨val_resultsç›®å½•ä¿å­˜è¯¦ç»†ç»“æœ
                epoch=epoch,
                fold=i+1,
                class_mapping=val_dataset.id2label,  # ä¼ é€’ç±»åˆ«æ˜ å°„
                localization_only=args.localization_only,  # ä¼ é€’å®šä½ä»»åŠ¡æ¨¡å¼å‚æ•°
                sample_level_fp=args.sample_level_fp  # ä¼ é€’æ ·æœ¬çº§åˆ«FPè®¡ç®—å‚æ•°
            )

            fold_result = {
                'fold': i + 1,
                'checkpoint': os.path.basename(checkpoint_path),
                'epoch': epoch,
                **eval_stats
            }
            all_results.append(fold_result)

            print(f"\nFold {i+1} è¯„ä¼°ç»“æœ:")
            if args.localization_only:
                print(
                    f"  Localization Precision: {eval_stats.get('precision', 0):.4f}")
                print(
                    f"  Localization Recall: {eval_stats.get('recall', 0):.4f}")
                print(
                    f"  Localization F1: {eval_stats.get('f1_score', 0):.4f}")
            else:
                print(
                    f"  Classification F1: {eval_stats.get('classification_f1', 0):.4f}")
                print(
                    f"  Classification AUC: {eval_stats.get('classification_auc', 0):.4f}")
                print(
                    f"  Detection Precision: {eval_stats.get('precision', 0):.4f}")
                print(f"  Detection Recall: {eval_stats.get('recall', 0):.4f}")
                print(f"  Detection F1: {eval_stats.get('f1_score', 0):.4f}")

            # ğŸ”¥ æ–°å¢ï¼šæ˜¾ç¤ºCDSçº§åˆ«æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if 'cds_level_metrics' in eval_stats:
                cds_metrics = eval_stats['cds_level_metrics']
                print(
                    f"  CDS Level Precision: {cds_metrics.get('precision', 0):.4f}")
                print(
                    f"  CDS Level Recall: {cds_metrics.get('recall', 0):.4f}")
                print(f"  CDS Level F1: {cds_metrics.get('f1_score', 0):.4f}")
                print(f"  CDS Level AUC: {cds_metrics.get('auc', 0):.4f}")
                print(f"  CDS Level TPR: {cds_metrics.get('tpr', 0):.4f}")
                print(f"  CDS Level FPR: {cds_metrics.get('fpr', 0):.4f}")
                print(
                    f"  CDS Level Stats: TP={cds_metrics.get('true_positive', 0)}, FP={cds_metrics.get('false_positive', 0)}, FN={cds_metrics.get('false_negative', 0)}, TN={cds_metrics.get('true_negative', 0)}")

            # ğŸ”¥ æ–°å¢ï¼šä¿å­˜è¯¦ç»†é¢„æµ‹ä¿¡æ¯
            if args.save_detailed_predictions:
                # ä»checkpointè·¯å¾„æå–æ¨¡å‹ä¿¡æ¯
                checkpoint_name = os.path.basename(checkpoint_path)
                checkpoint_name_without_ext = os.path.splitext(checkpoint_name)[
                    0]

                # ç”ŸæˆåŒ…å«è®­ç»ƒæ¨¡å‹ä¿¡æ¯çš„æ–‡ä»¶å
                detailed_filename = f"detailed_predictions_{checkpoint_name_without_ext}.json"
                detailed_file = os.path.join(
                    args.validation_output_dir,
                    detailed_filename
                )

                collect_detailed_predictions(
                    model=model,
                    data_loader=val_dataloader,
                    device=device,
                    class_mapping=val_dataset.id2label,
                    output_file=detailed_file,
                    model_info={
                        'checkpoint_path': checkpoint_path,
                        'checkpoint_name': checkpoint_name,
                        'epoch': epoch,
                        'model_name': args.backbone,
                        'num_classes': args.num_classes,
                        'num_queries': args.num_queries,
                        'localization_only': args.localization_only,
                        'no_confidence_filter': args.no_confidence_filter,
                        'sample_level_fp': args.sample_level_fp
                    },
                    max_tokens=args.max_tokens,  # ä¼ é€’max_tokenså‚æ•°
                    sample_level_fp=args.sample_level_fp  # ä¼ é€’sample_level_fpå‚æ•°
                )

        except Exception as e:
            print(f"è¯„ä¼° Fold {i+1} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
        finally:
            # æ¢å¤åŸå§‹è®¾ç½®
            if args.no_confidence_filter and original_eval_threshold is not None:
                import engine
                engine.EVAL_CONFIDENCE_THRESHOLD = original_eval_threshold
                engine.IOU_THRESHOLD = original_iou_threshold

                # æ¢å¤åå¤„ç†å™¨çš„åŸå§‹æ–¹æ³•
                if original_forward is not None:
                    from models.detr import PostProcess
                    PostProcess.forward = original_forward

    # æ±‡æ€»ç»“æœ
    if all_results:
        print(f"\n{'='*60}")
        print("æ‰€æœ‰Foldè¯„ä¼°ç»“æœæ±‡æ€»")
        print(f"{'='*60}")

        if args.localization_only:
            metrics = ['precision', 'recall', 'f1_score']
            metric_names = ['Localization Precision',
                            'Localization Recall', 'Localization F1']
        else:
            metrics = ['classification_f1', 'classification_auc',
                       'precision', 'recall', 'f1_score']
            metric_names = ['Classification F1', 'Classification AUC',
                            'Detection Precision', 'Detection Recall', 'Detection F1']

        for metric, metric_name in zip(metrics, metric_names):
            values = [r.get(metric, 0)
                      for r in all_results if r.get(metric) is not None]
            if values:
                mean_val = np.mean(values)
                std_val = np.std(values)
                print(f"{metric_name:20s}: {mean_val:.4f} Â± {std_val:.4f}")

        # ğŸ”¥ æ–°å¢ï¼šCDSçº§åˆ«æŒ‡æ ‡æ±‡æ€»
        print(f"\n{'='*60}")
        print("CDSçº§åˆ«è¯„ä¼°ç»“æœæ±‡æ€»")
        print(f"{'='*60}")

        cds_metrics = ['precision', 'recall', 'f1_score', 'auc', 'tpr', 'fpr']
        cds_metric_names = ['CDS Level Precision',
                            'CDS Level Recall', 'CDS Level F1', 'CDS Level AUC', 'CDS Level TPR', 'CDS Level FPR']

        for metric, metric_name in zip(cds_metrics, cds_metric_names):
            values = []
            for r in all_results:
                if 'cds_level_metrics' in r and r['cds_level_metrics'].get(metric) is not None:
                    values.append(r['cds_level_metrics'][metric])

            if values:
                mean_val = np.mean(values)
                std_val = np.std(values)
                print(f"{metric_name:20s}: {mean_val:.4f} Â± {std_val:.4f}")
            else:
                print(f"{metric_name:20s}: æ— æ•°æ®")

        # ä¿å­˜æ±‡æ€»ç»“æœ
        results_file = os.path.join(
            args.validation_output_dir, 'final_validation_results.json')
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"\næ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(
            f"è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {args.validation_output_dir}")

    print("\néªŒè¯å®Œæˆ!")


if __name__ == '__main__':
    main()


#     # ä½¿ç”¨å®½æ¾çš„IoUé˜ˆå€¼(0.1)å’Œç¦ç”¨ç½®ä¿¡åº¦è¿‡æ»¤
# python validate_final.py --no_confidence_filter

# # ä½¿ç”¨æ ‡å‡†è®¾ç½®(IoU=0.5, ç½®ä¿¡åº¦è¿‡æ»¤=0.3)
# python validate_final.py

# # æŒ‡å®šè‡ªå®šä¹‰éªŒè¯ç»“æœç›®å½•
# python validate_final.py --validation_output_dir ./my_validation_results

# # åªè®¡ç®—å®šä½ä»»åŠ¡ï¼Œå¿½ç•¥åˆ†ç±»
# python validate_final.py --localization_only --no_confidence_filter --validation_output_dir ./my_validation_results

# #  æ–°å¢ï¼šä¿å­˜è¯¦ç»†é¢„æµ‹ä¿¡æ¯
# python validate_final.py --save_detailed_predictions --detailed_predictions_file detailed_predictions.json --validation_output_dir ./Deformable_my_validation_results --localization_only --sample_level_fp

# #  æ–°å¢ï¼šæŒ‰æ ·æœ¬çº§åˆ«è®¡ç®—å‡é˜³æ€§ï¼ˆæ¨èç”¨äºè´Ÿæ ·æœ¬å¤„ç†ï¼‰
# python validate_final.py --save_detailed_predictions --sample_level_fp --validation_output_dir ./my_validation_results

# è´Ÿæ ·æœ¬å¤„ç†é€»è¾‘:
# - è´Ÿæ ·æœ¬: æ²¡æœ‰çœŸå®BGCåŒºåŸŸæˆ–åªæœ‰no-objectæ¡†çš„æ ·æœ¬
# - æŒ‰æ ·æœ¬çº§åˆ«: æ¯ä¸ªè´Ÿæ ·æœ¬æœ€å¤šè´¡çŒ®1ä¸ªFPï¼Œä¸ç®¡é¢„æµ‹äº†å¤šå°‘ä¸ªæ­£æ ·æœ¬æ¡†
# - æŒ‰é¢„æµ‹æ¡†çº§åˆ«: æ¯ä¸ªé¢„æµ‹çš„æ­£æ ·æœ¬æ¡†éƒ½ç®—ä½œ1ä¸ªFP
# - æ¨èä½¿ç”¨ --sample_level_fp å‚æ•°ï¼Œé¿å…è´Ÿæ ·æœ¬å¯¼è‡´çš„å‡é˜³æ€§é«˜ä¼°
# - æ³¨æ„: sample_level_fpå‚æ•°ä¼šå½±å“ä¸»è¦çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆprecision, recall, F1ï¼‰ï¼Œä¸ä»…ä»…æ˜¯åˆ†ææŠ¥å‘Š


# python3 /hy-tmp/scl/project/BGC-DETR/validate_final.py --localization_only --sample_level_fp --no_confidence_filter --validation_output_dir /hy-tmp/scl/project/BGC-DETR/my_validation_results

# python3 /hy-tmp/scl/project/BGC-DETR/validate_final.py --mapping_dir /hy-tmp/scl/project/BGC-DETR/data/test/genomes_split --output_dir /hy-tmp/scl/project/BGC-DETR/outputs --validation_output_dir /hy-tmp/scl/project/BGC-DETR/my_validation_results --localization_only --sample_level_fp --no_confidence_filter
