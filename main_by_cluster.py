# -*- coding: utf-8 -*-
# type: ignore
#!/usr/bin/env python3
"""
BGC-DETRä¸»è®­ç»ƒè„šæœ¬
å®ç°åŸºäºDETRæ¶æ„çš„ç”Ÿç‰©åŸºå› ç°‡æ£€æµ‹æ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯
æ”¯æŒ5æŠ˜äº¤å‰éªŒè¯å’Œåˆ†å¸ƒå¼è®­ç»ƒ
"""

# æ¨èçš„ Deformable ç‰ˆæœ¬è®­ç»ƒå‘½ä»¤ï¼ˆå•è¡Œï¼‰ï¼š
# python main_by_cluster.py --train_mapping_json data/converted/train/bgc_mapping.json --train_emb_dir data/converted/train/embeddings --val_mapping_json data/converted/val/bgc_mapping.json --val_emb_dir data/converted/val/embeddings --use_deformable --n_points 4 --n_levels 1 --num_queries 8 --epochs 500 --lr 1e-4 --weight_decay 1e-4 --set_cost_class 2 --set_cost_bbox 7 --set_cost_giou 3 --ce_loss_coef 1 --eos_coef 2 --focal_alpha 0.25 --output_dir outputs

import argparse
import json
import time
import datetime
import numpy as np
import torch
from torch.utils.data import DataLoader
from pathlib import Path
import random
from util.misc import (collate_fn, NestedTensor, get_args_parser, n_parameters,
                       init_distributed_mode, get_sha, save_on_master, is_main_process,
                       get_rank, get_world_size)
from data import BalancedClusterDataset
from models import build_model
from engine import train_one_epoch, evaluate
from torch.utils.data import DistributedSampler
import os
import torch.distributed as dist


def setup_for_distributed(is_master):
    """
    è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œç¦ç”¨éä¸»è¿›ç¨‹çš„æ‰“å°è¾“å‡º

    Args:
        is_master: æ˜¯å¦ä¸ºä¸»è¿›ç¨‹
    """
    import builtins
    builtin_print = builtins.print

    def print(*args, **kwargs):
        force = kwargs.pop('force', False)
        if is_master or force:
            builtin_print(*args, **kwargs)

    builtins.print = print


def main(args):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    å®ç°5æŠ˜äº¤å‰éªŒè¯çš„å®Œæ•´è®­ç»ƒæµç¨‹

    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # ===== è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³• =====
    # å¿…é¡»åœ¨ä»»ä½•CUDAæ“ä½œä¹‹å‰è®¾ç½®
    import torch.multiprocessing as mp
    try:
        mp.set_start_method('spawn', force=True)
        if is_main_process():
            print("å·²è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•ä¸º 'spawn'")
    except RuntimeError:
        if is_main_process():
            print("å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•å·²è®¾ç½®ï¼Œè·³è¿‡")

    # ===== åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½® =====
    # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        args.distributed = True
    else:
        rank = 0
        world_size = 1
        local_rank = 0
        args.distributed = False

    # è®¾ç½®å½“å‰è®¾å¤‡ï¼ˆå¼ºåˆ¶ä½¿ç”¨GPU 0ï¼‰
    if torch.cuda.is_available():
        torch.cuda.set_device(0)  # å¼ºåˆ¶ä½¿ç”¨GPU 0
        device = torch.device('cuda:0')
    else:
        device = torch.device('cpu')

    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    if args.distributed:
        # ä½¿ç”¨GLOOåç«¯ï¼Œå®ƒæ›´é€‚åˆåœ¨å•ä¸ªGPUä¸Šè¿è¡Œå¤šä¸ªè¿›ç¨‹
        dist.init_process_group(
            backend='gloo',  # ä½¿ç”¨GLOOè€Œä¸æ˜¯NCCL
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
        if rank == 0:
            print(f"| distributed init (rank {rank}): env://")
            print(f"| using GPU: 0")
            print(f"| world size: {world_size}")
            print(f"| device: {device}")
            print(f"| backend: gloo")

    # è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®éªŒå¯é‡ç°
    torch.manual_seed(args.seed + rank)
    np.random.seed(args.seed + rank)
    random.seed(args.seed + rank)

    # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥
    if args.distributed:
        dist.barrier()

    # åªæœ‰ä¸»è¿›ç¨‹æ‰“å°GPUä½¿ç”¨æƒ…å†µ
    if rank == 0:
        print("\nGPUä½¿ç”¨æƒ…å†µï¼š")
        print(f"ä½¿ç”¨çš„GPU: 0")
        print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(
            f"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        print(f"æ€»è¿›ç¨‹æ•°: {world_size}")

    if is_main_process():
        print("git:\n  {}\n".format(get_sha()))

    # æ£€æŸ¥å†»ç»“æƒé‡å‚æ•°
    if args.frozen_weights is not None:
        assert args.masks, "Frozen training is meant for segmentation only"
    if is_main_process():
        print(args)

    # ===== å‚æ•°éªŒè¯ =====
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†è®­ç»ƒé›†æˆ–éªŒè¯é›†çš„JSONæ–‡ä»¶
    if not (args.train_mapping_json or args.val_mapping_json or args.mapping_json):
        raise ValueError(
            "å¿…é¡»æŒ‡å®šè®­ç»ƒé›† (--train_mapping_json) æˆ–éªŒè¯é›† (--val_mapping_json) æˆ–å®Œæ•´çš„mappingæ–‡ä»¶ (--mapping_json)")

    # å¦‚æœåªæŒ‡å®šäº†mapping_jsonï¼Œåˆ™ä½¿ç”¨æ—§çš„äº¤å‰éªŒè¯æ¨¡å¼
    use_cross_validation = args.mapping_json is not None and (
        args.train_mapping_json is None and args.val_mapping_json is None)

    if use_cross_validation:
        if is_main_process():
            print("ä½¿ç”¨äº¤å‰éªŒè¯æ¨¡å¼ (legacy)")
        # ===== æ•°æ®é›†åŠ è½½å’Œåˆ’åˆ† =====
        # é¦–å…ˆåŠ è½½å®Œæ•´æ•°æ®é›†
        full_dataset = BalancedClusterDataset(
            args.mapping_json,
            args.emb_dir,
            max_tokens=args.max_tokens,
            balance_strategy='none',  # åˆå§‹åŠ è½½æ—¶ä¸è¿›è¡Œå¹³è¡¡
            binary_mode=args.binary_mode
        )
    else:
        if is_main_process():
            print("ä½¿ç”¨åˆ†ç¦»çš„æ•°æ®é›†æ¨¡å¼")
        full_dataset = None

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦ä¸ºç©ºï¼ˆä»…åœ¨äº¤å‰éªŒè¯æ¨¡å¼ä¸‹ï¼‰
    if use_cross_validation and full_dataset is not None:
        if len(full_dataset) == 0:
            raise ValueError("åŠ è½½çš„æ•°æ®é›†ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")

    if is_main_process() and use_cross_validation:
        print(f"åŸå§‹æ•°æ®é›†å¤§å°: {len(full_dataset)} ä¸ªæ ·æœ¬")

    if use_cross_validation:
        # ä½¿ç”¨å›ºå®šçš„éšæœºç§å­è¿›è¡Œåˆ’åˆ†
        generator = torch.Generator()
        generator.manual_seed(args.seed)

        # é¦–å…ˆç»Ÿè®¡æ¯ä¸ªBGCçš„é¢„æµ‹æ¡†æ•°é‡å’Œç±»åˆ«
        bgc_box_counts = {}
        bgc_classes = {}
        for bgc_id in full_dataset.bgc_ids:
            bgc_box_counts[bgc_id] = len(full_dataset.mapping[bgc_id])
            # æ”¶é›†è¿™ä¸ªBGCåŒ…å«çš„æ‰€æœ‰ç±»åˆ«
            classes = set()
            for region in full_dataset.mapping[bgc_id]:
                classes.add(region['type'])
            bgc_classes[bgc_id] = classes

        # æŒ‰é¢„æµ‹æ¡†æ•°é‡å¯¹BGCè¿›è¡Œæ’åº
        sorted_bgcs = sorted(bgc_box_counts.items(),
                             key=lambda x: x[1], reverse=True)

        # å‡†å¤‡5-foldäº¤å‰éªŒè¯
        n_folds = 5
        folds = [[] for _ in range(n_folds)]

        # åˆ†å±‚æŠ½æ ·ï¼šç¡®ä¿æ¯ä¸ªfoldéƒ½åŒ…å«æ‰€æœ‰ç±»åˆ«
        # é¦–å…ˆæŒ‰ç±»åˆ«åˆ†ç»„BGC
        class_to_bgcs = {}
        for bgc_id, classes in bgc_classes.items():
            for cls in classes:
                if cls not in class_to_bgcs:
                    class_to_bgcs[cls] = []
                class_to_bgcs[cls].append(bgc_id)

        # å¯¹æ¯ä¸ªç±»åˆ«ï¼Œå°†å…¶BGCå¹³å‡åˆ†é…åˆ°å„ä¸ªfold
        for cls, bgc_list in class_to_bgcs.items():
            # éšæœºæ‰“ä¹±BGCåˆ—è¡¨
            random.shuffle(bgc_list)
            # å¹³å‡åˆ†é…åˆ°å„ä¸ªfold
            for i, bgc_id in enumerate(bgc_list):
                fold_idx = i % n_folds
                folds[fold_idx].append(bgc_id)

        # å»é‡ï¼ˆå› ä¸ºä¸€ä¸ªBGCå¯èƒ½åŒ…å«å¤šä¸ªç±»åˆ«ï¼‰
        for i in range(n_folds):
            folds[i] = list(set(folds[i]))

        # æ‰“å°æ¯ä¸ªfoldçš„ç»Ÿè®¡ä¿¡æ¯
        if is_main_process():
            print(f"\næ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡ï¼š")
            print(f"- åŸå§‹æ•°æ®é›†å¤§å°: {len(full_dataset)} ä¸ªBGCæ ·æœ¬")
            total_boxes = sum(bgc_box_counts.values())
            print(f"- æ€»é¢„æµ‹æ¡†æ•°é‡: {total_boxes} ä¸ª")

        for i, fold_bgcs in enumerate(folds):
            fold_boxes = sum(bgc_box_counts[bgc] for bgc in fold_bgcs)
            if is_main_process():
                print(f"\nFold {i+1} ç»Ÿè®¡ï¼š")
                print(f"- BGCæ ·æœ¬æ•°: {len(fold_bgcs)}")
                print(
                    f"- é¢„æµ‹æ¡†æ•°é‡: {fold_boxes} ({fold_boxes/total_boxes*100:.1f}%)")

    # å­˜å‚¨æ‰€æœ‰foldçš„ç»“æœ
    all_fold_results = []

    if use_cross_validation:
        # ===== 5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒå¾ªç¯ =====
        fold_range = range(n_folds)
        n_folds_to_run = n_folds
        display_n_folds = n_folds
    else:
        # ===== ç›´æ¥è®­ç»ƒæ¨¡å¼ =====
        fold_range = range(1)  # åªè¿è¡Œä¸€æ¬¡
        n_folds_to_run = 1
        display_n_folds = 1
        folds = [None]  # å ä½ç¬¦
        bgc_box_counts = {}  # åˆå§‹åŒ–ä¸ºç©ºå­—å…¸

    for fold_idx in fold_range:
        if is_main_process():
            print(f"\n{'='*50}")
            print(f"å¼€å§‹è®­ç»ƒ Fold {fold_idx + 1}/{display_n_folds}")
            print(f"{'='*50}")

        if use_cross_validation:
            # å‡†å¤‡å½“å‰foldçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†
            val_bgcs = folds[fold_idx]  # å½“å‰foldä½œä¸ºéªŒè¯é›†
            train_bgcs = []
            for i in range(n_folds):
                if i != fold_idx:
                    train_bgcs.extend(folds[i])  # å…¶ä»–foldä½œä¸ºè®­ç»ƒé›†

            # åˆ›å»ºè®­ç»ƒé›†ï¼ˆä½¿ç”¨ç±»åˆ«å¹³è¡¡ï¼‰
            dataset_train = BalancedClusterDataset(
                args.mapping_json,
                args.emb_dir,
                max_tokens=args.max_tokens,
                balance_strategy=args.balance_strategy,
                seq_id_list=train_bgcs,
                binary_mode=args.binary_mode
            )

            # åˆ›å»ºéªŒè¯é›†ï¼ˆä¸ä½¿ç”¨ç±»åˆ«å¹³è¡¡ï¼‰
            dataset_val = BalancedClusterDataset(
                args.mapping_json,
                args.emb_dir,
                max_tokens=args.max_tokens,
                balance_strategy='none',
                seq_id_list=val_bgcs,
                binary_mode=args.binary_mode
            )

            if is_main_process():
                print(f"\nFold {fold_idx + 1} æ•°æ®é›†åˆ’åˆ†ï¼š")
                print(f"- è®­ç»ƒé›†BGCæ•°é‡: {len(train_bgcs)}")
                print(f"- éªŒè¯é›†BGCæ•°é‡: {len(val_bgcs)}")
        else:
            # ä½¿ç”¨åˆ†ç¦»çš„æ•°æ®é›†æ¨¡å¼
            if is_main_process():
                print(f"\nç›´æ¥è®­ç»ƒæ¨¡å¼ï¼š")

            # åˆ›å»ºè®­ç»ƒé›†
            if args.train_mapping_json:
                # ä½¿ç”¨æŒ‡å®šçš„è®­ç»ƒé›†embeddingsè·¯å¾„
                train_emb_dir = args.train_emb_dir if args.train_emb_dir else args.emb_dir

                dataset_train = BalancedClusterDataset(
                    args.train_mapping_json,
                    train_emb_dir,
                    max_tokens=args.max_tokens,
                    balance_strategy=args.balance_strategy,
                    binary_mode=args.binary_mode
                )
                if is_main_process():
                    print(f"- è®­ç»ƒé›†: {args.train_mapping_json}")
                    print(f"- è®­ç»ƒé›†embeddings: {train_emb_dir}")
                    print(f"- è®­ç»ƒé›†BGCæ•°é‡: {len(dataset_train)}")
            else:
                dataset_train = None
                if is_main_process():
                    print("- æœªæŒ‡å®šè®­ç»ƒé›†ï¼Œå°†åªè¿›è¡ŒéªŒè¯")

            # åˆ›å»ºéªŒè¯é›†
            if args.val_mapping_json:
                # ä½¿ç”¨æŒ‡å®šçš„éªŒè¯é›†embeddingsè·¯å¾„
                val_emb_dir = args.val_emb_dir if args.val_emb_dir else args.emb_dir

                dataset_val = BalancedClusterDataset(
                    args.val_mapping_json,
                    val_emb_dir,
                    max_tokens=args.max_tokens,
                    balance_strategy='none',
                    binary_mode=args.binary_mode
                )
                if is_main_process():
                    print(f"- éªŒè¯é›†: {args.val_mapping_json}")
                    print(f"- éªŒè¯é›†embeddings: {val_emb_dir}")
                    print(f"- éªŒè¯é›†BGCæ•°é‡: {len(dataset_val)}")
            else:
                dataset_val = None
                if is_main_process():
                    print("- æœªæŒ‡å®šéªŒè¯é›†ï¼Œå°†åªè¿›è¡Œè®­ç»ƒ")

        if is_main_process():
            print(f"\nFold {fold_idx + 1} æ•°æ®é›†å¤§å°ï¼š")
            if use_cross_validation:
                print(
                    f"- è®­ç»ƒé›†ï¼š{len(dataset_train)} ä¸ªBGCæ ·æœ¬ï¼Œ{sum(bgc_box_counts[bgc] for bgc in train_bgcs)} ä¸ªé¢„æµ‹æ¡†")
                print(
                    f"- éªŒè¯é›†ï¼š{len(dataset_val)} ä¸ªBGCæ ·æœ¬ï¼Œ{sum(bgc_box_counts[bgc] for bgc in val_bgcs)} ä¸ªé¢„æµ‹æ¡†")
            else:
                if dataset_train:
                    print(f"- è®­ç»ƒé›†ï¼š{len(dataset_train)} ä¸ªBGCæ ·æœ¬")
                if dataset_val:
                    print(f"- éªŒè¯é›†ï¼š{len(dataset_val)} ä¸ªBGCæ ·æœ¬")
                else:
                    print("- æ— å¯ç”¨æ•°æ®é›†ä¿¡æ¯")

        # ä»æ•°æ®é›†ä¸­è·å–å®é™…çš„ç±»åˆ«æ•°é‡
        # ä¼˜å…ˆä½¿ç”¨è®­ç»ƒé›†ï¼Œå¦åˆ™ä½¿ç”¨éªŒè¯é›†
        reference_dataset = dataset_train if dataset_train else dataset_val
        num_classes = len(reference_dataset.label2id)  # æ•°æ®é›†ä¸­å·²ç»åŒ…å«äº†no-objectç±»
        if is_main_process():
            print(f"ä»æ•°æ®é›†è·å–çš„ç±»åˆ«æ•°é‡: {num_classes}")
            print(f"ç±»åˆ«æ˜ å°„: {reference_dataset.label2id}")
        args.num_classes = num_classes

        # ===== æ•°æ®åŠ è½½å™¨è®¾ç½® =====
        if dataset_train is not None:
            if args.distributed:
                sampler_train = DistributedSampler(dataset_train)
            else:
                sampler_train = torch.utils.data.RandomSampler(dataset_train)

            batch_sampler_train = torch.utils.data.BatchSampler(
                sampler_train, args.batch_size, drop_last=True)

            data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train,
                                           collate_fn=collate_fn, num_workers=args.num_workers)
        else:
            data_loader_train = None

        if dataset_val is not None:
            if args.distributed:
                sampler_val = DistributedSampler(dataset_val, shuffle=False)
            else:
                sampler_val = torch.utils.data.SequentialSampler(dataset_val)

            data_loader_val = DataLoader(dataset_val, args.batch_size, sampler=sampler_val,
                                         drop_last=False, collate_fn=collate_fn, num_workers=args.num_workers)
        else:
            data_loader_val = None

        # ===== æ¨¡å‹æ„å»º =====
        # ä¸ºæ¯ä¸ªfoldåˆ›å»ºæ–°çš„æ¨¡å‹
        model, criterion, postprocessors = build_model(args)
        model.to(device)
        criterion.to(device)

        # ===== æ£€æŸ¥æ¨¡å‹åˆå§‹åŒ–çŠ¶æ€ =====
        if is_main_process() and fold_idx == 0:  # åªåœ¨ç¬¬ä¸€ä¸ªfoldæ£€æŸ¥ä¸€æ¬¡
            print(f"\nğŸ” æ¨¡å‹åˆå§‹åŒ–çŠ¶æ€æ£€æŸ¥:")
            print(f"  â€¢ åˆ†ç±»å¤´æƒé‡shape: {model.class_embed.weight.shape}")
            print(
                f"  â€¢ åˆ†ç±»å¤´åç½®: {[f'{b:.3f}' for b in model.class_embed.bias.tolist()]}")
            print(f"  â€¢ num_queries: {model.num_queries}")
            print(f"  â€¢ num_classes: {model.num_classes}")
            print(f"  â€¢ eos_coef: {args.eos_coef}")
            print(f"  â€¢ focal_alpha: {args.focal_alpha}")
            print(f"  â€¢ å­¦ä¹ ç‡: {args.lr}")

            # æµ‹è¯•ä¸€ä¸ªå°æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­
            print(f"  â€¢ æ­£åœ¨æµ‹è¯•å‰å‘ä¼ æ’­...")
            with torch.no_grad():
                # åˆ›å»ºä¸€ä¸ªå‡çš„è¾“å…¥æ¥æµ‹è¯•
                test_input = torch.randn(2, args.hidden_dim, 128, 1).to(device)
                test_mask = torch.zeros(2, 128, dtype=torch.bool).to(device)
                from util.misc import NestedTensor
                test_nested = NestedTensor(test_input, test_mask)

                test_output = model(test_nested)
                # [2, num_queries, num_classes]
                test_logits = test_output['pred_logits']
                test_probs = test_logits.softmax(-1)
                test_labels = test_logits.argmax(-1)

                print(f"  â€¢ æµ‹è¯•è¾“å‡ºshape: {test_logits.shape}")
                print(
                    f"  â€¢ é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ: {torch.unique(test_labels, return_counts=True)}")
                print(
                    f"  â€¢ å„ç±»åˆ«å¹³å‡æ¦‚ç‡: {[f'{p:.3f}' for p in test_probs.mean(dim=(0,1)).tolist()]}")

                if len(torch.unique(test_labels)) == 1:
                    print(f"  âš ï¸  è­¦å‘Š: æ¨¡å‹åˆå§‹åŒ–ååªé¢„æµ‹ä¸€ä¸ªç±»åˆ«!")
                else:
                    print(f"  âœ… æ¨¡å‹åˆå§‹åŒ–æ­£å¸¸ï¼Œé¢„æµ‹å¤šä¸ªç±»åˆ«")
            print()

        model_without_ddp = model
        if args.distributed:
            try:
                model = torch.nn.parallel.DistributedDataParallel(
                    model, device_ids=[local_rank], find_unused_parameters=True)
                model_without_ddp = model.module
                if is_main_process():
                    print(f"Fold {fold_idx + 1} DDPåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                if is_main_process():
                    print(f"Fold {fold_idx + 1} DDPåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                    print("é™çº§ä¸ºå•GPUè®­ç»ƒæ¨¡å¼")
                # å¦‚æœDDPåˆå§‹åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å•GPUæ¨¡å¼
                args.distributed = False
                model_without_ddp = model

        # ===== ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½® =====
        # ä¸ºæ¯ä¸ªfoldåˆ›å»ºæ–°çš„ä¼˜åŒ–å™¨
        param_dicts = [
            {"params": [p for n, p in model_without_ddp.named_parameters()
             if "backbone" not in n and p.requires_grad]},
            {
                "params": [p for n, p in model_without_ddp.named_parameters() if "backbone" in n and p.requires_grad],
                "lr": args.lr_backbone,
            },
        ]
        optimizer = torch.optim.AdamW(param_dicts, lr=args.lr,
                                      weight_decay=args.weight_decay)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)

        lr_scheduler.last_epoch = -1  # é‡ç½®è°ƒåº¦å™¨çŠ¶æ€ï¼Œç¡®ä¿æ¯ä¸ªfoldéƒ½ä»åˆå§‹å­¦ä¹ ç‡å¼€å§‹
        if is_main_process():
            print(f"Fold {fold_idx + 1} å­¦ä¹ ç‡è°ƒåº¦å™¨å·²é‡ç½®ï¼Œåˆå§‹å­¦ä¹ ç‡: {args.lr}")

        # ===== é¢„è®­ç»ƒæƒé‡åŠ è½½ =====
        # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if args.frozen_weights is not None:
            checkpoint = torch.load(args.frozen_weights, map_location='cpu')
            model_without_ddp.detr.load_state_dict(checkpoint['model'])

        output_dir = Path(args.output_dir)
        if args.resume:
            if args.resume.startswith('https'):
                checkpoint = torch.hub.load_state_dict_from_url(
                    args.resume, map_location='cpu', check_hash=True)
            else:
                checkpoint = torch.load(args.resume, map_location='cpu')
            model_without_ddp.load_state_dict(checkpoint['model'])
            if not args.eval and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer'])
                lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])
                args.start_epoch = checkpoint['epoch'] + 1

        # ===== è¯„ä¼°æ¨¡å¼ =====
        if args.eval:
            if data_loader_val is not None:
                val_stats = evaluate(model, criterion, postprocessors,
                                     data_loader_val, device, args.output_dir,
                                     epoch=0, fold=fold_idx+1,
                                     class_mapping=dataset_val.id2label,
                                     binary_mode=args.binary_mode,
                                     bg_class_index=args.num_classes - 1)
                if args.output_dir:
                    save_on_master(val_stats, output_dir / "validation.pth")
                all_fold_results.append(val_stats)
            else:
                if is_main_process():
                    print("è­¦å‘Š: æœªæŒ‡å®šéªŒè¯é›†ï¼Œè·³è¿‡è¯„ä¼°æ¨¡å¼")
            continue

        if data_loader_train is not None:
            if is_main_process():
                print("Start training")
            start_time = time.time()

            # ===== è®­ç»ƒå¾ªç¯ =====
        # ç”¨äºè·Ÿè¸ªæœ€ä½³æ¨¡å‹
        best_f1 = 0.0
        best_epoch = 0

        # ç”¨äºè·Ÿè¸ªæ¯save_intervalè½®ä¸­çš„æœ€ä½³æ¨¡å‹
        interval_best_f1 = 0.0
        interval_best_epoch = 0
        interval_best_model_state = None
        interval_best_optimizer_state = None
        interval_best_lr_scheduler_state = None

        if is_main_process():
            print(f"\nå¼€å§‹è®­ç»ƒ Fold {fold_idx + 1}")
            print(f"è®­ç»ƒå‚æ•°ï¼š")
            print(f"- å­¦ä¹ ç‡: {args.lr}")
            print(f"- æ‰¹æ¬¡å¤§å°: {args.batch_size}")
            print(f"- ä¿å­˜é—´éš”: {args.save_interval}è½®")
            print(f"- è®­ç»ƒè½®æ•°: {args.epochs}")

        for epoch in range(args.start_epoch, args.epochs):
            try:
                if args.distributed:
                    sampler_train.set_epoch(epoch)

                # è®­ç»ƒä¸€ä¸ªepoch
                if is_main_process():
                    print(f"\nå¼€å§‹è®­ç»ƒ epoch {epoch}...")
                train_stats = train_one_epoch(
                    model, criterion, data_loader_train, optimizer, device, epoch,
                    args.clip_max_norm, fold=fold_idx+1)
                lr_scheduler.step()

                # è¯„ä¼°å½“å‰æ¨¡å‹
                if is_main_process():
                    print(f"\nå¼€å§‹è¯„ä¼° epoch {epoch}...")
                if args.ema:
                    # åˆå§‹åŒ–æˆ–æ›´æ–° EMA æƒé‡
                    if 'ema' not in locals():
                        ema = {k: v.detach().clone()
                               for k, v in model_without_ddp.state_dict().items()}
                    else:
                        with torch.no_grad():
                            for k, v in model_without_ddp.state_dict().items():
                                if k in ema:
                                    ema[k].mul_(args.ema_decay).add_(
                                        v.detach(), alpha=1.0 - args.ema_decay)
                                else:
                                    ema[k] = v.detach().clone()
                    # å¤‡ä»½ã€æ›¿æ¢ã€è¯„ä¼°ã€è¿˜åŸ
                    backup = {k: v.detach().clone()
                              for k, v in model_without_ddp.state_dict().items()}
                    model_without_ddp.load_state_dict(ema, strict=False)
                    val_stats = evaluate(model, criterion, postprocessors,
                                         data_loader_val, device, args.output_dir, epoch,
                                         fold=fold_idx+1, class_mapping=dataset_val.id2label,
                                         binary_mode=args.binary_mode,
                                         bg_class_index=args.num_classes - 1)
                    model_without_ddp.load_state_dict(backup, strict=False)
                else:
                    val_stats = evaluate(model, criterion, postprocessors,
                                         data_loader_val, device, args.output_dir, epoch,
                                         fold=fold_idx+1, class_mapping=dataset_val.id2label,
                                         binary_mode=args.binary_mode,
                                         bg_class_index=args.num_classes - 1)
            except Exception as e:
                if is_main_process():
                    print(f"Epoch {epoch} è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                    print("è·³è¿‡å½“å‰epochï¼Œç»§ç»­ä¸‹ä¸€ä¸ª")
                continue

            # ===== æ¨¡å‹è¯„ä¼°å’Œä¿å­˜ =====
            # ä»éªŒè¯é›†æŒ‡æ ‡ä¸­ç›´æ¥è·å–F1åˆ†æ•°ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼‰
            if is_main_process():
                print(f"\nè¯„ä¼°å®Œæˆï¼Œå¼€å§‹è®¡ç®—F1åˆ†æ•°...")
                val_f1 = val_stats.get('f1_score', 0.0)
                print(f"\nå½“å‰epoch {epoch} çš„éªŒè¯é›†F1åˆ†æ•°: {val_f1:.4f}")

                # æ›´æ–°å…¨å±€æœ€ä½³æ¨¡å‹
                if val_f1 > best_f1:
                    best_f1 = val_f1
                    best_epoch = epoch
                    print(f"å‘ç°æ–°çš„å…¨å±€æœ€ä½³F1åˆ†æ•°: {best_f1:.4f} (epoch {best_epoch})")
                    # ä¿å­˜å…¨å±€æœ€ä½³æ¨¡å‹
                    if args.output_dir:
                        print("æ­£åœ¨ä¿å­˜å…¨å±€æœ€ä½³æ¨¡å‹...")
                        checkpoint_path = output_dir / \
                            f'checkpoint_fold_{fold_idx + 1}.pth'
                        save_on_master({
                            'model': model_without_ddp.state_dict(),
                            'optimizer': optimizer.state_dict(),
                            'lr_scheduler': lr_scheduler.state_dict(),
                            'epoch': epoch,
                            'args': args,
                            'best_f1': best_f1,
                        }, checkpoint_path)
                        print("å…¨å±€æœ€ä½³æ¨¡å‹ä¿å­˜å®Œæˆ")

                # æ›´æ–°å½“å‰é—´éš”å†…çš„æœ€ä½³æ¨¡å‹
                if val_f1 > interval_best_f1:
                    interval_best_f1 = val_f1
                    interval_best_epoch = epoch
                    interval_best_model_state = model_without_ddp.state_dict()
                    interval_best_optimizer_state = optimizer.state_dict()
                    interval_best_lr_scheduler_state = lr_scheduler.state_dict()
                    print(
                        f"æ›´æ–°å½“å‰{args.save_interval}è½®å†…çš„æœ€ä½³F1åˆ†æ•°: {interval_best_f1:.4f} (epoch {interval_best_epoch})")

                # ===== å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ =====
                # æ¯save_intervalè½®ä¿å­˜ä¸€æ¬¡å†å²æ£€æŸ¥ç‚¹ï¼Œä¿å­˜è¯¥é—´éš”å†…F1åˆ†æ•°æœ€é«˜çš„æ¨¡å‹
                if args.output_dir and (epoch + 1) % args.save_interval == 0:
                    print(f"å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ (æ¯{args.save_interval}è½®ï¼Œä¿å­˜è¯¥é—´éš”å†…æœ€ä½³æ¨¡å‹)...")
                    checkpoint_path = output_dir / \
                        f'checkpoint_fold_{fold_idx + 1}_epoch_{epoch}_best_in_interval.pth'

                    # ä½¿ç”¨è¯¥é—´éš”å†…æœ€ä½³æ¨¡å‹çš„çŠ¶æ€è¿›è¡Œä¿å­˜
                    save_on_master({
                        'model': interval_best_model_state,
                        'optimizer': interval_best_optimizer_state,
                        'lr_scheduler': interval_best_lr_scheduler_state,
                        'epoch': interval_best_epoch,
                        'args': args,
                        'best_f1': interval_best_f1,
                        'best_epoch': interval_best_epoch,
                        'val_f1': interval_best_f1,
                        'interval_start_epoch': epoch - args.save_interval + 1,
                        'interval_end_epoch': epoch,
                    }, checkpoint_path)
                    print(f"é—´éš”å†…æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}")
                    print(
                        f"è¯¥é—´éš”å†…æœ€ä½³F1åˆ†æ•°: {interval_best_f1:.4f} (epoch {interval_best_epoch})")

                    # é‡ç½®é—´éš”å†…æœ€ä½³æ¨¡å‹è·Ÿè¸ª
                    interval_best_f1 = 0.0
                    interval_best_epoch = 0
                    interval_best_model_state = None
                    interval_best_optimizer_state = None
                    interval_best_lr_scheduler_state = None

                print(f"\nå‡†å¤‡å¼€å§‹ä¸‹ä¸€ä¸ªepoch {epoch + 1}...")

            # ===== åˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥ =====
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†å½“å‰epoch
            if args.distributed:
                try:
                    dist.barrier()
                except Exception as e:
                    if is_main_process():
                        print(f"Epoch {epoch} åŒæ­¥æ—¶å‡ºé”™: {str(e)}")
                    # å¦‚æœåŒæ­¥å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒ

        # ===== Foldç»“æœè®°å½• =====
        # è®°å½•å½“å‰foldçš„ç»“æœï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼‰
        if is_main_process():
            all_fold_results.append({
                'fold': fold_idx + 1,
                'best_epoch': best_epoch,
                'best_f1': best_f1
            })
            print(f"\nFold {fold_idx + 1} è®­ç»ƒå®Œæˆ")
            print(f"- æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
            print(f"- æœ€ä½³epoch: {best_epoch}")

        else:
            # æ²¡æœ‰è®­ç»ƒæ•°æ®çš„å¤„ç†
            if is_main_process():
                print("è·³è¿‡è®­ç»ƒé˜¶æ®µï¼ˆæœªæŒ‡å®šè®­ç»ƒé›†ï¼‰")
            best_f1 = 0.0
            best_epoch = 0

        # ===== Foldç»“æœè®°å½• =====
        # è®°å½•å½“å‰foldçš„ç»“æœï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼‰
        if is_main_process():
            all_fold_results.append({
                'fold': fold_idx + 1,
                'best_epoch': best_epoch,
                'best_f1': best_f1
            })
            if data_loader_train is not None:
                print(f"\nFold {fold_idx + 1} è®­ç»ƒå®Œæˆ")
                print(f"- æœ€ä½³F1åˆ†æ•°: {best_f1:.4f}")
                print(f"- æœ€ä½³epoch: {best_epoch}")
            else:
                print(f"\nFold {fold_idx + 1} å®Œæˆï¼ˆä»…éªŒè¯æ¨¡å¼ï¼‰")

            # åœ¨foldä¹‹é—´è¿›è¡ŒåŒæ­¥å’Œæ¸…ç†
        if args.distributed:
            try:
                # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å®Œæˆäº†å½“å‰fold
                dist.barrier()

                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # é‡ç½®epochè®¡æ•°å™¨ï¼Œä¸ºä¸‹ä¸€ä¸ªfoldåšå‡†å¤‡
                args.start_epoch = 0

                if is_main_process():
                    print(f"Fold {fold_idx + 1} æ¸…ç†å®Œæˆï¼Œå‡†å¤‡å¼€å§‹ä¸‹ä¸€ä¸ªfold...")

            except Exception as e:
                if is_main_process():
                    print(f"Fold {fold_idx + 1} æ¸…ç†æ—¶å‡ºé”™: {str(e)}")
                    print("ç»§ç»­ä¸‹ä¸€ä¸ªfold...")
                # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªfold
                # é‡ç½®epochè®¡æ•°å™¨ï¼Œä¸ºä¸‹ä¸€ä¸ªfoldåšå‡†å¤‡
                args.start_epoch = 0
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

    # ===== æœ€ç»ˆç»“æœç»Ÿè®¡ =====
    # è®¡ç®—å¹¶æ‰“å°æ‰€æœ‰foldçš„å¹³å‡ç»“æœï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œï¼‰
    if is_main_process():
        if use_cross_validation:
            avg_f1 = sum(fold['best_f1']
                         for fold in all_fold_results) / n_folds
            print("\n" + "="*50)
            print("äº¤å‰éªŒè¯æœ€ç»ˆç»“æœï¼š")
            print("="*50)
            print(f"å¹³å‡æœ€ä½³F1åˆ†æ•°: {avg_f1:.4f}")
            for fold in all_fold_results:
                print(
                    f"Fold {fold['fold']}: æœ€ä½³F1åˆ†æ•° = {fold['best_f1']:.4f} (epoch {fold['best_epoch']})")
        else:
            print("\n" + "="*50)
            print("ç›´æ¥è®­ç»ƒæœ€ç»ˆç»“æœï¼š")
            print("="*50)
            for fold in all_fold_results:
                print(
                    f"æœ€ä½³F1åˆ†æ•°: {fold['best_f1']:.4f} (epoch {fold['best_epoch']})")

    # ä¿å­˜æ‰€æœ‰foldçš„ç»“æœ
    if args.output_dir:
        if use_cross_validation:
            results_path = output_dir / 'cross_validation_results.json'
        else:
            results_path = output_dir / 'direct_training_results.json'
        save_on_master(all_fold_results, results_path)

    # è®¡ç®—æ€»æ—¶é—´
    if data_loader_train is not None and 'start_time' in locals():
        total_time = time.time() - start_time
        total_time_str = str(datetime.timedelta(seconds=int(total_time)))
        print('Training time {}'.format(total_time_str))
    else:
        print('Total time: N/A (no training performed)')


if __name__ == '__main__':
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(
        'DETR training and evaluation script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    if getattr(args, 'binary_mode', False):
        print("âš ï¸  binary_mode å¼€å…³å·²å¯ç”¨ï¼šç›®å‰ä»…è¾“å‡ºæç¤ºï¼Œåç»­é˜¶æ®µå°†é€æ­¥åŠ å…¥å®Œæ•´é€»è¾‘ã€‚")
    main(args)


# ===== è¿è¡Œå‘½ä»¤ç¤ºä¾‹ =====

# å¤šåˆ†ç±»è®­ç»ƒï¼ˆé»˜è®¤ï¼‰
# CUDA_VISIBLE_DEVICES=0 python main_by_cluster.py --train_mapping_json data/converted/train/bgc_mapping.json --train_emb_dir data/converted/train/embeddings --val_mapping_json data/converted/val/bgc_mapping.json --val_emb_dir data/converted/val/embeddings --output_dir outputs_multiclass --batch_size 128 --lr 1e-4 --dropout 0.2 --balance_strategy weighted
# # å•ä¸€ mapping.jsonï¼Œå†…éƒ¨æ‰§è¡Œ 8:2 åˆ’åˆ†ï¼ˆlegacy æ¨¡å¼ï¼‰
# CUDA_VISIBLE_DEVICES=0 python main_by_cluster.py --mapping_json data/new_data/bgc_mapping.json --emb_dir data/new_data/embeddings --output_dir outputs_multiclass --batch_size 128 --lr 1e-4 --dropout 0.2 --balance_strategy weighted

# å¤šåˆ†ç±»éªŒè¯
# python validate_final.py --mapping_json data/new_data/bgc_mapping.json --emb_dir data/new_data/embeddings --output_dir outputs_multiclass --validation_output_dir my_validation_results_multiclass

# äºŒåˆ†ç±»è®­ç»ƒï¼ˆBGC vs èƒŒæ™¯ï¼‰
# CUDA_VISIBLE_DEVICES=0 python main_by_cluster.py --train_mapping_json data/converted/train/bgc_mapping.json --train_emb_dir data/converted/train/embeddings --val_mapping_json data/converted/val/bgc_mapping.json --val_emb_dir data/converted/val/embeddings --binary_mode --output_dir outputs_binary --batch_size 128 --lr 1e-4 --dropout 0.2 --balance_strategy weighted
# #CUDA_VISIBLE_DEVICES=0 python main_by_cluster.py --mapping_json data/new_data/bgc_mapping.json --emb_dir data/new_data/embeddings --output_dir outputs_binary --batch_size 128 --lr 1e-4 --dropout 0.2 --balance_strategy weighted --binary_mode

# äºŒåˆ†ç±»éªŒè¯
# python validate_final.py --mapping_json data/new_data/bgc_mapping.json --emb_dir data/new_data/embeddings --output_dir outputs_binary --validation_output_dir my_validation_results_binary --binary_mode
